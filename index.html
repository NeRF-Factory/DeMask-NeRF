<html lang="en"><head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <title>Mask-NeRF: Neural Radiance Fields for Masked Image Restoration</title>
</head>

<body>
<div class="container">
    <br>
    <div style="text-align: center;">
        <h1>Mask-NeRF:</h1>
        <h3>Neural Radiance Fields for Masked Image Restoration</h3>
        <!--        <div style="margin-bottom: 10px;">-->
        <!--            <span style="margin-right: 10px; font-size: 1.2em;">Michael Niemeyer</span> <span style="font-size: 1.2em;">Andreas Geiger</span>-->
        <!--        </div>-->
        <!--        <div>-->
        <!--        <span style="margin-right: 10px; font-size: 1.2em;">Max Planck Institute for Intelligent Systems and University-->
        <!--          of TÃ¼bingen</span>-->
        <!--        </div>-->
        <!--        <div>-->
        <!--            <span style="margin-right: 10px; font-size: 1.2em;">CVPR 2021 (oral, <strong>best paper award</strong>)</span>-->
        <!--        </div>        -->
        <div>
            <span style="margin-right: 10px; font-size: 1.2em;">Anonymous CVPR submission</span>
        </div>
    </div>
    <div class="text-center" style="margin-top: 60px; margin-bottom: 20px;">
        <img src="./pipeline.png" class="img-fluid" alt="pipeline" width="60%">
    </div>
    <div class="text-center" style="font-size: 1.5em; margin-bottom: 30px;">
<!--        <a href="http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf" target="_blank" style="margin-right: 20px;">[Paper]</a>-->
<!--        <a href="http://www.cvlibs.net/publications/Niemeyer2021CVPR_supplementary.pdf" target="_blank" style="margin-right: 20px;">[Supplementary]</a>-->
        <a href="https://github.com/autonomousvision/giraffe" target="_blank" style="margin-right: 20px;">[Code]</a>
<!--        <a href="http://autonomousvision.github.io/giraffe" target="_blank" style="margin-right: 20px;">[Blog]</a>-->
        <a href="http://www.youtube.com/watch?v=fIaDXC-qRSg&amp;vq=hd1080&amp;autoplay=1" target="_blank" style="margin-right: 20px;">[Video]</a>
<!--        <a href="https://m-niemeyer.github.io/slides/talks/giraffe/index.html" target="_blank" style="margin-right: 20px;">[Interactive Slides]</a>-->
<!--        <a href="https://www.youtube.com/watch?v=scnXyCSMJF4" target="_blank">[Talk]</a>-->
    </div>
    <div>
        <h2 class="text-center">
            Abstract
        </h2>
        <p style="font-style: italic;">
            Recently, neural radiance fields (NeRF) have demonstrated remarkable performance in novel view synthesis.
            However, there is little work on recovering 3D scenes with
            interference, which is ubiquitous in natural scene capture
            and can significantly degrade the performance of NeRFs.
            In this paper, we present Mask-NeRF, a neural rendering
            method for restoring masked realistic scene images. Observing that randomly emitting rays to pixels in NeRFs can-
            not effectively learn complex textures in images, we propose an image entropy-based ray allocation strategy to
            properly distribute the emitted rays, which enables Mask-NeRF to fuse useful information from images of different
            views. In addition, we develop an iteration restoration
            mechanism to progressively restore the masked images in
            a self-training way. Our method can be easily plugged into
            most existing NeRF methods, and applicable to many subsequent computer vision tasks. As existing datasets do not
            support NeRF-based masked image restoration, we build
            the light-field-forward-facing masked dataset (LLFF-M in
            short) based on the LLFF dataset to simulate interference
            scenarios. Extensive experiments on LLFF-M show that
            Mask-NeRF is superior to the counterparts in masked image restoration.
        </p>
    </div>
    <div style="margin-top:10px;">
        <h2 class="text-center">
            Video
        </h2>
        <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/fIaDXC-qRSg" allowfullscreen=""></iframe>
        </div>
    </div>
    <div style="margin-top:20px;">
        <h2 class="text-center">
            Results
        </h2>
        <h4>Comparison Against original NeRF on LLFF-M dataset</h4>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="fern_nerf.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="text-center">
                    <p>Single-Object Translation for 2D-based GAN</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="fern_ours.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="text-center">
                    <p>Single-Object Translation for Our Method</p>
                </div>
            </div>
        </div>
        <p>We can perform more complex operations like circular translations or adding objects at test time.</p>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/clevr/translation_circle_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Circular Translations</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/clevr/add_objects.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Add Objects (Trained on Two-Object Scenes)</p>
                </div>
            </div>
        </div>
        <h4>Controllable Scene Generation</h4>
        <p>We show more examples where we control the scene during image synthesis.</p>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/cars/rotation_object_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Rotate Object</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/celebahq/rotate_celebahq.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Rotate Object</p>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/cars/translation_horizontal_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Horizontal Translation</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/cars/translation_vertical_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Vertical Translation</p>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/cars/cars_app.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Change Object Appearance</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/celebahq/app_celebahq.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Change Object Appearance</p>
                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/cars/bg_cars.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Change Background Appearance</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/churches/interpolate_appearance_bg_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Change Background Appearance</p>
                </div>
            </div>
        </div>
        <h4>Out-of-Distribution Generalization</h4>
        <p>As our model disentangles individual objects, we are able to generate out of distribution samples. For example, we can increase the horizontal translation range.</p>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/cars/translation_horizontal_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Training Distribution</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/outof/translation_horizontal_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Out-Of-Distribution</p>
                </div>
            </div>
        </div>
        <p>We can increase the depth translation range.</p>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/cars/translation_vertical_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Training Distribution</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/outof/translation_vertical_sm.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Out-Of-Distribution</p>
                </div>
            </div>
        </div>
        <p>We can add more objects at test time.</p>
        <div class="row">
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/clevr/add_objects.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Out-Of Distribution (Trained On Two-Object Scenes)</p>
                </div>
            </div>
            <div class="col-md-6 col-sm-6 col-xs-12 gallery">
                <div class="embed-responsive embed-responsive-16by9">
                    <video controls="" loop="" muted="" autoplay="" class="embed-responsive-item">
                        <source src="gfx/outof/add_objects.webm" type="video/webm">
                    </video>
                </div>
                <div class="text-center">
                    <p>Out-Of Distribution (Trained On One-Object Scenes)</p>
                </div>
            </div>
        </div>
    </div>
    <div>
        <h2 class="text-center">
            Citation
        </h2>
        <p>
            If you want to cite our work, please use:
        </p>
        <pre>        @inproceedings{Niemeyer2020GIRAFFE,
          author    = {Michael Niemeyer and Andreas Geiger},
          title     = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},
          booktitle   = {Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
          year      = {2021},
        }
      </pre>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>



</div></body></html>
